{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjwq6pS-kFz"
      },
      "source": [
        "# Stock NeurIPS2018 Part 2. Train\n",
        "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
        "\n",
        "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
        "\n",
        "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT-zXutMgqOS"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D0vEcPxSJ8hI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
            "Requirement already satisfied: swig in /root/miniconda3/envs/py310/lib/python3.10/site-packages (4.2.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
            "Requirement already satisfied: wrds in /root/miniconda3/envs/py310/lib/python3.10/site-packages (3.2.0)\n",
            "Requirement already satisfied: numpy<1.27,>=1.26 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from wrds) (1.26.4)\n",
            "Requirement already satisfied: packaging<23.3 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from wrds) (23.2)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from wrds) (2.2.2)\n",
            "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from wrds) (2.9.9)\n",
            "Requirement already satisfied: scipy<1.13,>=1.12 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from wrds) (1.12.0)\n",
            "Requirement already satisfied: sqlalchemy<2.1,>=2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from wrds) (2.0.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
            "Requirement already satisfied: pyportfolioopt in /root/miniconda3/envs/py310/lib/python3.10/site-packages (1.5.5)\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pyportfolioopt) (1.5.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pyportfolioopt) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.19 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pyportfolioopt) (2.2.2)\n",
            "Requirement already satisfied: scipy<2.0,>=1.3 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pyportfolioopt) (1.12.0)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.4)\n",
            "Requirement already satisfied: ecos>=2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.0.13)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.9.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (3.2.4.post2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2024.1)\n",
            "Requirement already satisfied: qdldl in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.1.7.post2)\n",
            "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyportfolioopt) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
            "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-j2_n5s0z\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-j2_n5s0z\n",
            "  fatal: unable to access 'https://github.com/AI4Finance-Foundation/FinRL.git/': GnuTLS recv error (-54): Error in the pull function.\n",
            "  warning: Clone succeeded, but checkout failed.\n",
            "  You can inspect what was checked out with 'git status'\n",
            "  and retry the checkout with 'git checkout -f HEAD'\n",
            "\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/AI4Finance-Foundation/FinRL.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-j2_n5s0z\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mgit clone --\u001b[0m\u001b[32mfilter\u001b[0m\u001b[32m=\u001b[0m\u001b[32mblob\u001b[0m\u001b[32m:none --quiet \u001b[0m\u001b[4;32mhttps://github.com/AI4Finance-Foundation/FinRL.git\u001b[0m\u001b[32m \u001b[0m\u001b[32m/tmp/\u001b[0m\u001b[32mpip-req-build-j2_n5s0z\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m128\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "## install required packages\n",
        "%pip install swig\n",
        "%pip install wrds\n",
        "%pip install pyportfolioopt\n",
        "## install finrl library\n",
        "%pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from stable_baselines3.common.logger import configure\n",
        "from finrl import config_tickers\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWrSrQv3i0Ng"
      },
      "source": [
        "# Part 2. Build A Market Environment in OpenAI Gym-style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiHhM2U-XBMZ"
      },
      "source": [
        "![rl_diagram_transparent_bg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeneTRdyZDvy"
      },
      "source": [
        "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
        "\n",
        "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
        "\n",
        "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKyZejI0fmp1"
      },
      "source": [
        "## Read data\n",
        "\n",
        "We first read the .csv file of our training data into dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data.csv')\n",
        "\n",
        "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
        "# it has the columns and index in the form that could be make into the environment. \n",
        "# Then you can comment and skip the following two lines.\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw95ZMicgEyi"
      },
      "source": [
        "## Construct the environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WZ6-9q2gq9S"
      },
      "source": [
        "Calculate and specify the parameters we need for constructing the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "4817e063-400a-416e-f8f2-4b1c4d9c8408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ],
      "source": [
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7We-q73jjaFQ"
      },
      "source": [
        "## Environment for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "a733ecdf-d857-40f5-b399-4325c7ead299"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMNR5nHjh1iz"
      },
      "source": [
        "# Part 3: Train DRL Agents\n",
        "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
        "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "2794a094-a916-448c-ead1-6e20184dde2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to results/a2c\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "f29cf145-e3b5-4e59-f64d-5921462a8f81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 73         |\n",
            "|    iterations         | 100        |\n",
            "|    time_elapsed       | 6          |\n",
            "|    total_timesteps    | 500        |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 99         |\n",
            "|    policy_loss        | -107       |\n",
            "|    reward             | 0.44829795 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 9.83       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 11         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -81.7      |\n",
            "|    reward             | -2.6805062 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.31       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 98        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 15        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -144      |\n",
            "|    reward             | 6.0222664 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 11.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 102       |\n",
            "|    iterations         | 400       |\n",
            "|    time_elapsed       | 19        |\n",
            "|    total_timesteps    | 2000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.252    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 399       |\n",
            "|    policy_loss        | -31.3     |\n",
            "|    reward             | 0.4159002 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 6.84      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 105        |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 23         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 831        |\n",
            "|    reward             | -16.504211 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 621        |\n",
            "--------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 107          |\n",
            "|    iterations         | 600          |\n",
            "|    time_elapsed       | 27           |\n",
            "|    total_timesteps    | 3000         |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.1        |\n",
            "|    explained_variance | 0.0239       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 599          |\n",
            "|    policy_loss        | 130          |\n",
            "|    reward             | -0.111250125 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 12.3         |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 109        |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 31         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -94.6      |\n",
            "|    reward             | -2.5264087 |\n",
            "|    std                | 0.997      |\n",
            "|    value_loss         | 5.76       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 110        |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 36         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 39.7       |\n",
            "|    reward             | -3.3780408 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 1.78       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 111        |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 40         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 262        |\n",
            "|    reward             | -2.7616417 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 47.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 112        |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 44         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | -83.4      |\n",
            "|    reward             | -6.8069644 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.86       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 112        |\n",
            "|    iterations         | 1100       |\n",
            "|    time_elapsed       | 48         |\n",
            "|    total_timesteps    | 5500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | -0.0155    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1099       |\n",
            "|    policy_loss        | -83.4      |\n",
            "|    reward             | -1.0537128 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 13.5       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 113        |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 52         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.0614     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -71.2      |\n",
            "|    reward             | 0.24483411 |\n",
            "|    std                | 0.998      |\n",
            "|    value_loss         | 3.13       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 113       |\n",
            "|    iterations         | 1300      |\n",
            "|    time_elapsed       | 57        |\n",
            "|    total_timesteps    | 6500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1299      |\n",
            "|    policy_loss        | -20.3     |\n",
            "|    reward             | -3.14824  |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 3.59      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 113      |\n",
            "|    iterations         | 1400     |\n",
            "|    time_elapsed       | 61       |\n",
            "|    total_timesteps    | 7000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.1    |\n",
            "|    explained_variance | -0.0204  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 1399     |\n",
            "|    policy_loss        | 110      |\n",
            "|    reward             | 1.29295  |\n",
            "|    std                | 0.998    |\n",
            "|    value_loss         | 8.85     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 114        |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 65         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 108        |\n",
            "|    reward             | -0.8264961 |\n",
            "|    std                | 0.996      |\n",
            "|    value_loss         | 7.74       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 114         |\n",
            "|    iterations         | 1600        |\n",
            "|    time_elapsed       | 69          |\n",
            "|    total_timesteps    | 8000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 1599        |\n",
            "|    policy_loss        | 143         |\n",
            "|    reward             | -0.08224123 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 14.5        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 114       |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 74        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | -0.0681   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -71.5     |\n",
            "|    reward             | 1.7476935 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 17.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 114       |\n",
            "|    iterations         | 1800      |\n",
            "|    time_elapsed       | 78        |\n",
            "|    total_timesteps    | 9000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | -0.00622  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1799      |\n",
            "|    policy_loss        | -73       |\n",
            "|    reward             | 0.6324917 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 3.43      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 114        |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 82         |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | -39.1      |\n",
            "|    reward             | 0.20377894 |\n",
            "|    std                | 0.995      |\n",
            "|    value_loss         | 1.21       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 115       |\n",
            "|    iterations         | 2000      |\n",
            "|    time_elapsed       | 86        |\n",
            "|    total_timesteps    | 10000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1999      |\n",
            "|    policy_loss        | 16.9      |\n",
            "|    reward             | 0.4712003 |\n",
            "|    std                | 0.994     |\n",
            "|    value_loss         | 0.645     |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 115      |\n",
            "|    iterations         | 2100     |\n",
            "|    time_elapsed       | 91       |\n",
            "|    total_timesteps    | 10500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41      |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2099     |\n",
            "|    policy_loss        | 21.9     |\n",
            "|    reward             | 1.352369 |\n",
            "|    std                | 0.995    |\n",
            "|    value_loss         | 0.994    |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 115       |\n",
            "|    iterations         | 2200      |\n",
            "|    time_elapsed       | 95        |\n",
            "|    total_timesteps    | 11000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | -0.127    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2199      |\n",
            "|    policy_loss        | 114       |\n",
            "|    reward             | -6.038066 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 12.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 115       |\n",
            "|    iterations         | 2300      |\n",
            "|    time_elapsed       | 99        |\n",
            "|    total_timesteps    | 11500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2299      |\n",
            "|    policy_loss        | -1.98e+03 |\n",
            "|    reward             | -9.18198  |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 2.27e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 115       |\n",
            "|    iterations         | 2400      |\n",
            "|    time_elapsed       | 103       |\n",
            "|    total_timesteps    | 12000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2399      |\n",
            "|    policy_loss        | 50.6      |\n",
            "|    reward             | 0.5940837 |\n",
            "|    std                | 0.996     |\n",
            "|    value_loss         | 4.08      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 115        |\n",
            "|    iterations         | 2500       |\n",
            "|    time_elapsed       | 108        |\n",
            "|    total_timesteps    | 12500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2499       |\n",
            "|    policy_loss        | -27.7      |\n",
            "|    reward             | -2.3551543 |\n",
            "|    std                | 0.999      |\n",
            "|    value_loss         | 1.16       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 115      |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 112      |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.2    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 39.1     |\n",
            "|    reward             | 3.257917 |\n",
            "|    std                | 1        |\n",
            "|    value_loss         | 1.29     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 115         |\n",
            "|    iterations         | 2700        |\n",
            "|    time_elapsed       | 116         |\n",
            "|    total_timesteps    | 13500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2699        |\n",
            "|    policy_loss        | -131        |\n",
            "|    reward             | -0.68936497 |\n",
            "|    std                | 0.999       |\n",
            "|    value_loss         | 11.4        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 2800      |\n",
            "|    time_elapsed       | 120       |\n",
            "|    total_timesteps    | 14000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2799      |\n",
            "|    policy_loss        | 542       |\n",
            "|    reward             | 1.7470409 |\n",
            "|    std                | 0.998     |\n",
            "|    value_loss         | 214       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 124       |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -82.8     |\n",
            "|    reward             | 1.8873638 |\n",
            "|    std                | 0.997     |\n",
            "|    value_loss         | 4.71      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 116         |\n",
            "|    iterations         | 3000        |\n",
            "|    time_elapsed       | 128         |\n",
            "|    total_timesteps    | 15000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | -0.0509     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2999        |\n",
            "|    policy_loss        | 44.8        |\n",
            "|    reward             | -0.20238155 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 1.13        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 116         |\n",
            "|    iterations         | 3100        |\n",
            "|    time_elapsed       | 133         |\n",
            "|    total_timesteps    | 15500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3099        |\n",
            "|    policy_loss        | 97.6        |\n",
            "|    reward             | -0.21961628 |\n",
            "|    std                | 0.998       |\n",
            "|    value_loss         | 7.96        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 116        |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 137        |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.1      |\n",
            "|    explained_variance | 0.00809    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | -138       |\n",
            "|    reward             | -2.8526402 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 25.9       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 116        |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 141        |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.2      |\n",
            "|    explained_variance | -0.0614    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | -20.7      |\n",
            "|    reward             | -1.5480262 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 1.41       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 145       |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 95.9      |\n",
            "|    reward             | 4.0947967 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 26.2      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 116         |\n",
            "|    iterations         | 3500        |\n",
            "|    time_elapsed       | 149         |\n",
            "|    total_timesteps    | 17500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | 0.00705     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 3499        |\n",
            "|    policy_loss        | 153         |\n",
            "|    reward             | -0.27995098 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 18.5        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 154       |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.2     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -25.9     |\n",
            "|    reward             | 1.2630055 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 1.26      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 3700      |\n",
            "|    time_elapsed       | 158       |\n",
            "|    total_timesteps    | 18500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3699      |\n",
            "|    policy_loss        | 182       |\n",
            "|    reward             | 3.1950305 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 28.6      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 162       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 43.6      |\n",
            "|    reward             | 3.7015467 |\n",
            "|    std                | 1         |\n",
            "|    value_loss         | 3.65      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 166       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -65       |\n",
            "|    reward             | 0.6211153 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 3.09      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 171       |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -310      |\n",
            "|    reward             | 5.4044576 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 54.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 116        |\n",
            "|    iterations         | 4100       |\n",
            "|    time_elapsed       | 175        |\n",
            "|    total_timesteps    | 20500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.0468    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4099       |\n",
            "|    policy_loss        | -66.1      |\n",
            "|    reward             | -0.4074066 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 5.33       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 4200      |\n",
            "|    time_elapsed       | 179       |\n",
            "|    total_timesteps    | 21000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4199      |\n",
            "|    policy_loss        | -33.8     |\n",
            "|    reward             | 1.0019952 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.13      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 183      |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -37.1    |\n",
            "|    reward             | 4.216202 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 1.98     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 188       |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.0877    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 131       |\n",
            "|    reward             | 1.1873147 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 15.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 116        |\n",
            "|    iterations         | 4500       |\n",
            "|    time_elapsed       | 192        |\n",
            "|    total_timesteps    | 22500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4499       |\n",
            "|    policy_loss        | 354        |\n",
            "|    reward             | -1.7338417 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 81.6       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 196       |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 104       |\n",
            "|    reward             | 3.1525953 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 12        |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 116         |\n",
            "|    iterations         | 4700        |\n",
            "|    time_elapsed       | 200         |\n",
            "|    total_timesteps    | 23500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.2       |\n",
            "|    explained_variance | -0.123      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4699        |\n",
            "|    policy_loss        | -195        |\n",
            "|    reward             | -0.08115613 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 23.5        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 116        |\n",
            "|    iterations         | 4800       |\n",
            "|    time_elapsed       | 205        |\n",
            "|    total_timesteps    | 24000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4799       |\n",
            "|    policy_loss        | -162       |\n",
            "|    reward             | -0.7530768 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 18.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 4900       |\n",
            "|    time_elapsed       | 209        |\n",
            "|    total_timesteps    | 24500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4899       |\n",
            "|    policy_loss        | -54.8      |\n",
            "|    reward             | -0.7365128 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.04       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 5000       |\n",
            "|    time_elapsed       | 213        |\n",
            "|    total_timesteps    | 25000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4999       |\n",
            "|    policy_loss        | -89.3      |\n",
            "|    reward             | -2.2345834 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 7.06       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 5100        |\n",
            "|    time_elapsed       | 217         |\n",
            "|    total_timesteps    | 25500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5099        |\n",
            "|    policy_loss        | 136         |\n",
            "|    reward             | -0.31054717 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 44.9        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 5200      |\n",
            "|    time_elapsed       | 222       |\n",
            "|    total_timesteps    | 26000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5199      |\n",
            "|    policy_loss        | -367      |\n",
            "|    reward             | 1.7650476 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 103       |\n",
            "-------------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6176322.11\n",
            "total_reward: 5176322.11\n",
            "total_cost: 35641.59\n",
            "total_trades: 52452\n",
            "Sharpe: 0.984\n",
            "=================================\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 5300       |\n",
            "|    time_elapsed       | 226        |\n",
            "|    total_timesteps    | 26500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5299       |\n",
            "|    policy_loss        | -68.1      |\n",
            "|    reward             | 0.30185336 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.32       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 5400       |\n",
            "|    time_elapsed       | 230        |\n",
            "|    total_timesteps    | 27000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5399       |\n",
            "|    policy_loss        | -70        |\n",
            "|    reward             | -1.1869549 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.82       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 234       |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 310       |\n",
            "|    reward             | 3.6181183 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 60.7      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 5600      |\n",
            "|    time_elapsed       | 239       |\n",
            "|    total_timesteps    | 28000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5599      |\n",
            "|    policy_loss        | -139      |\n",
            "|    reward             | 2.8170679 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 20.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 5700      |\n",
            "|    time_elapsed       | 243       |\n",
            "|    total_timesteps    | 28500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5699      |\n",
            "|    policy_loss        | -42.9     |\n",
            "|    reward             | 1.5013282 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 8.59      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 247       |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -0.0174   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -33.2     |\n",
            "|    reward             | 0.8849819 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.89      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 5900        |\n",
            "|    time_elapsed       | 251         |\n",
            "|    total_timesteps    | 29500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5899        |\n",
            "|    policy_loss        | 61.8        |\n",
            "|    reward             | -0.86370087 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 4.15        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 6000        |\n",
            "|    time_elapsed       | 256         |\n",
            "|    total_timesteps    | 30000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5999        |\n",
            "|    policy_loss        | 147         |\n",
            "|    reward             | -0.16920196 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 14          |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 116      |\n",
            "|    iterations         | 6100     |\n",
            "|    time_elapsed       | 260      |\n",
            "|    total_timesteps    | 30500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.5    |\n",
            "|    explained_variance | 2.38e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6099     |\n",
            "|    policy_loss        | -96.1    |\n",
            "|    reward             | -8.09379 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 14.6     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 116        |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 265        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | -122       |\n",
            "|    reward             | -1.1193475 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 13.9       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 116       |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 269       |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 1.16e+03  |\n",
            "|    reward             | 7.0545774 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.06e+03  |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 273       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 41.5      |\n",
            "|    reward             | 1.9486883 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.33      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 6500      |\n",
            "|    time_elapsed       | 277       |\n",
            "|    total_timesteps    | 32500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6499      |\n",
            "|    policy_loss        | 62        |\n",
            "|    reward             | -5.473836 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 10.4      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 6600       |\n",
            "|    time_elapsed       | 281        |\n",
            "|    total_timesteps    | 33000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6599       |\n",
            "|    policy_loss        | -24        |\n",
            "|    reward             | 0.14508653 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 3.51       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 286       |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | -1.49e+03 |\n",
            "|    reward             | -10.29194 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.39e+03  |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 6800     |\n",
            "|    time_elapsed       | 290      |\n",
            "|    total_timesteps    | 34000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 6799     |\n",
            "|    policy_loss        | -382     |\n",
            "|    reward             | 0.44143  |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 82.6     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 6900       |\n",
            "|    time_elapsed       | 294        |\n",
            "|    total_timesteps    | 34500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6899       |\n",
            "|    policy_loss        | -427       |\n",
            "|    reward             | -10.241172 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 164        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 7000        |\n",
            "|    time_elapsed       | 298         |\n",
            "|    total_timesteps    | 35000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | -0.0116     |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6999        |\n",
            "|    policy_loss        | 44.1        |\n",
            "|    reward             | -0.45898107 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 1.45        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 303       |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | -33.3     |\n",
            "|    reward             | 1.6200018 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.35      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 7200       |\n",
            "|    time_elapsed       | 307        |\n",
            "|    total_timesteps    | 36000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7199       |\n",
            "|    policy_loss        | -292       |\n",
            "|    reward             | -1.6064115 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 46.7       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 7300       |\n",
            "|    time_elapsed       | 311        |\n",
            "|    total_timesteps    | 36500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7299       |\n",
            "|    policy_loss        | -115       |\n",
            "|    reward             | -2.0223186 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 12.5       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 315        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 281        |\n",
            "|    reward             | -10.023635 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 52.4       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 319        |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | 290        |\n",
            "|    reward             | -3.9870293 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 71.6       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 7600       |\n",
            "|    time_elapsed       | 324        |\n",
            "|    total_timesteps    | 38000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.0407    |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7599       |\n",
            "|    policy_loss        | -126       |\n",
            "|    reward             | 0.96909016 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 8.18       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 328       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | -66.5     |\n",
            "|    reward             | 2.4158041 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 2.82      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 7800        |\n",
            "|    time_elapsed       | 332         |\n",
            "|    total_timesteps    | 39000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7799        |\n",
            "|    policy_loss        | -84         |\n",
            "|    reward             | -0.47822315 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 6.17        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 336       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 441       |\n",
            "|    reward             | 2.6675744 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 146       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 340        |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | -265       |\n",
            "|    reward             | 0.33679518 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 48.7       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 345       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | 210       |\n",
            "|    reward             | 8.51873   |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 59.6      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 8200        |\n",
            "|    time_elapsed       | 349         |\n",
            "|    total_timesteps    | 41000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8199        |\n",
            "|    policy_loss        | -21.2       |\n",
            "|    reward             | -0.45113897 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.874       |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 8300      |\n",
            "|    time_elapsed       | 353       |\n",
            "|    total_timesteps    | 41500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | 4.17e-06  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8299      |\n",
            "|    policy_loss        | -17.1     |\n",
            "|    reward             | -2.344584 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.398     |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 8400        |\n",
            "|    time_elapsed       | 357         |\n",
            "|    total_timesteps    | 42000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8399        |\n",
            "|    policy_loss        | 5.58        |\n",
            "|    reward             | -0.99927014 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 0.179       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 8500       |\n",
            "|    time_elapsed       | 361        |\n",
            "|    total_timesteps    | 42500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8499       |\n",
            "|    policy_loss        | -33.7      |\n",
            "|    reward             | 0.54120874 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.91       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 8600       |\n",
            "|    time_elapsed       | 365        |\n",
            "|    total_timesteps    | 43000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8599       |\n",
            "|    policy_loss        | 108        |\n",
            "|    reward             | -12.942829 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 28.2       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 370       |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | -3.71     |\n",
            "|    reward             | 0.7916671 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.912     |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 374       |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.0026    |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | -44.6     |\n",
            "|    reward             | 0.6572692 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.31      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 378       |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.5     |\n",
            "|    explained_variance | -0.0047   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | -21       |\n",
            "|    reward             | 1.2493141 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 0.948     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 9000       |\n",
            "|    time_elapsed       | 382        |\n",
            "|    total_timesteps    | 45000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8999       |\n",
            "|    policy_loss        | -12.8      |\n",
            "|    reward             | -0.8812681 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.14       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 117       |\n",
            "|    iterations         | 9100      |\n",
            "|    time_elapsed       | 386       |\n",
            "|    total_timesteps    | 45500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 0.00178   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9099      |\n",
            "|    policy_loss        | 25.7      |\n",
            "|    reward             | 1.2357236 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 1.31      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 9200     |\n",
            "|    time_elapsed       | 391      |\n",
            "|    total_timesteps    | 46000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9199     |\n",
            "|    policy_loss        | -4.64    |\n",
            "|    reward             | 6.910831 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 3.56     |\n",
            "------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 9300     |\n",
            "|    time_elapsed       | 395      |\n",
            "|    total_timesteps    | 46500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.4    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9299     |\n",
            "|    policy_loss        | -116     |\n",
            "|    reward             | 1.387503 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 9.49     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 9400       |\n",
            "|    time_elapsed       | 399        |\n",
            "|    total_timesteps    | 47000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9399       |\n",
            "|    policy_loss        | 134        |\n",
            "|    reward             | -0.9776756 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 11.4       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 9500        |\n",
            "|    time_elapsed       | 403         |\n",
            "|    total_timesteps    | 47500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.4       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9499        |\n",
            "|    policy_loss        | -44.2       |\n",
            "|    reward             | -0.14308462 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 2.26        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 9600        |\n",
            "|    time_elapsed       | 407         |\n",
            "|    total_timesteps    | 48000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.5       |\n",
            "|    explained_variance | 0.000143    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9599        |\n",
            "|    policy_loss        | -678        |\n",
            "|    reward             | -0.41902286 |\n",
            "|    std                | 1.01        |\n",
            "|    value_loss         | 221         |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 117        |\n",
            "|    iterations         | 9700       |\n",
            "|    time_elapsed       | 412        |\n",
            "|    total_timesteps    | 48500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9699       |\n",
            "|    policy_loss        | 144        |\n",
            "|    reward             | -2.0923386 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 14.1       |\n",
            "--------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 117      |\n",
            "|    iterations         | 9800     |\n",
            "|    time_elapsed       | 416      |\n",
            "|    total_timesteps    | 49000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 0.00187  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9799     |\n",
            "|    policy_loss        | 110      |\n",
            "|    reward             | 3.02979  |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 31.7     |\n",
            "------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 9900        |\n",
            "|    time_elapsed       | 420         |\n",
            "|    total_timesteps    | 49500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9899        |\n",
            "|    policy_loss        | -14.3       |\n",
            "|    reward             | -0.26173118 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.489       |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 117         |\n",
            "|    iterations         | 10000       |\n",
            "|    time_elapsed       | 424         |\n",
            "|    total_timesteps    | 50000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.6       |\n",
            "|    explained_variance | 5.96e-08    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9999        |\n",
            "|    policy_loss        | 41.4        |\n",
            "|    reward             | -0.58077085 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 1.77        |\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c, \n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "M2YadjfnLwgt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tCDa78rqfO_a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3792810.76\n",
            "total_reward: 2792810.76\n",
            "total_cost: 5299.09\n",
            "total_trades: 44684\n",
            "Sharpe: 0.708\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 146       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 37.1      |\n",
            "|    critic_loss     | 431       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 2.9625297 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 291       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 11.8      |\n",
            "|    critic_loss     | 28        |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 2.9625297 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5044807.93\n",
            "total_reward: 4044807.93\n",
            "total_cost: 1180.10\n",
            "total_trades: 43430\n",
            "Sharpe: 0.794\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 79        |\n",
            "|    time_elapsed    | 436       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -1.08     |\n",
            "|    critic_loss     | 7.82      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 2.9625297 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 80        |\n",
            "|    time_elapsed    | 577       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | -0.844    |\n",
            "|    critic_loss     | 6.53      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 2.9625297 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg, \n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "y5D5PFUhMzSV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to results/ppo\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "| time/              |              |\n",
            "|    fps             | 121          |\n",
            "|    iterations      | 1            |\n",
            "|    time_elapsed    | 16           |\n",
            "|    total_timesteps | 2048         |\n",
            "| train/             |              |\n",
            "|    reward          | -0.036096916 |\n",
            "-------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 122         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 33          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019218441 |\n",
            "|    clip_fraction        | 0.231       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.000271    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.14        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0269     |\n",
            "|    reward               | 0.6625561   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 13.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 49          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012040381 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00541     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 31.4        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0168     |\n",
            "|    reward               | -1.5304433  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 56.7        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 124          |\n",
            "|    iterations           | 4            |\n",
            "|    time_elapsed         | 65           |\n",
            "|    total_timesteps      | 8192         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0144221615 |\n",
            "|    clip_fraction        | 0.155        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -41.3        |\n",
            "|    explained_variance   | 0.00841      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 9.8          |\n",
            "|    n_updates            | 30           |\n",
            "|    policy_gradient_loss | -0.0153      |\n",
            "|    reward               | 3.1869254    |\n",
            "|    std                  | 1.01         |\n",
            "|    value_loss           | 45.1         |\n",
            "------------------------------------------\n",
            "day: 2892, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3127481.92\n",
            "total_reward: 2127481.92\n",
            "total_cost: 331291.56\n",
            "total_trades: 79965\n",
            "Sharpe: 0.655\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 125        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 81         |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01043624 |\n",
            "|    clip_fraction        | 0.0841     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.3      |\n",
            "|    explained_variance   | 0.00428    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 6.45       |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0181    |\n",
            "|    reward               | 2.8940384  |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 16.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 98          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017687764 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.00577    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.5        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0202     |\n",
            "|    reward               | 1.8380961   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 44.7        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 115        |\n",
            "|    total_timesteps      | 14336      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01602402 |\n",
            "|    clip_fraction        | 0.166      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.4      |\n",
            "|    explained_variance   | 0.00165    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 39.4       |\n",
            "|    n_updates            | 60         |\n",
            "|    policy_gradient_loss | -0.0159    |\n",
            "|    reward               | 1.1084496  |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 58.1       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 131         |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019849688 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.106      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.5         |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.013      |\n",
            "|    reward               | 0.05267552  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 22.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 147         |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015594047 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.5       |\n",
            "|    explained_variance   | -0.0153     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.5        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    reward               | 0.5827399   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 32.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 163         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025303036 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.00865    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16.9        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0198     |\n",
            "|    reward               | 0.2918841   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 39.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 180         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014617417 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.00118    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.9        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | 1.2746822   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 78.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 196         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023034938 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.0226     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.92        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0193     |\n",
            "|    reward               | 0.14986631  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 13.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 212         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02069604  |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | 0.0179      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.7        |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    reward               | -0.25920928 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 58.7        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 125          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 228          |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.026609236  |\n",
            "|    clip_fraction        | 0.294        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -41.8        |\n",
            "|    explained_variance   | 0.0145       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 82.2         |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00744     |\n",
            "|    reward               | -0.050404355 |\n",
            "|    std                  | 1.02         |\n",
            "|    value_loss           | 90.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 245         |\n",
            "|    total_timesteps      | 30720       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022944622 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.0223      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.6        |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.011      |\n",
            "|    reward               | 3.034336    |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 35.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 261         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022257702 |\n",
            "|    clip_fraction        | 0.226       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.0248     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 30.1        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    reward               | -0.07580721 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 60.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 17         |\n",
            "|    time_elapsed         | 280        |\n",
            "|    total_timesteps      | 34816      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0357844  |\n",
            "|    clip_fraction        | 0.337      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42        |\n",
            "|    explained_variance   | -0.00722   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 42.3       |\n",
            "|    n_updates            | 160        |\n",
            "|    policy_gradient_loss | -0.0128    |\n",
            "|    reward               | 0.94821453 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 79.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 296         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019304458 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.0172      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 21.6        |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0183     |\n",
            "|    reward               | -0.7280321  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 65.8        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5433566.09\n",
            "total_reward: 4433566.09\n",
            "total_cost: 314377.80\n",
            "total_trades: 77878\n",
            "Sharpe: 0.898\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 312        |\n",
            "|    total_timesteps      | 38912      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01891324 |\n",
            "|    clip_fraction        | 0.171      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.1      |\n",
            "|    explained_variance   | 0.0316     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 9.22       |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.0156    |\n",
            "|    reward               | -1.1147597 |\n",
            "|    std                  | 1.03       |\n",
            "|    value_loss           | 22.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 328         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015094639 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.033       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 44.1        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.0123     |\n",
            "|    reward               | -0.33826074 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 134         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 347         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023043368 |\n",
            "|    clip_fraction        | 0.189       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.00937     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24.5        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0116     |\n",
            "|    reward               | -12.221691  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 87          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 363         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021534966 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | -0.0577     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16.1        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0197     |\n",
            "|    reward               | 3.3311608   |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 31.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 379         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015359257 |\n",
            "|    clip_fraction        | 0.126       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.00495     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.1        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0144     |\n",
            "|    reward               | 0.68813866  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 86.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 395         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030816326 |\n",
            "|    clip_fraction        | 0.239       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.0122      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.2        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0145     |\n",
            "|    reward               | 7.931518    |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 105         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 124          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 412          |\n",
            "|    total_timesteps      | 51200        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.022385025  |\n",
            "|    clip_fraction        | 0.185        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -42.3        |\n",
            "|    explained_variance   | 0.00266      |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 60           |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.0113      |\n",
            "|    reward               | 0.0051176627 |\n",
            "|    std                  | 1.04         |\n",
            "|    value_loss           | 117          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 428         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039191872 |\n",
            "|    clip_fraction        | 0.278       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | 0.00864     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.4        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | -0.28766206 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 23.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 444         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027830768 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.00498     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 50.6        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0122     |\n",
            "|    reward               | -0.21182822 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 121         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 461         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022278806 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | -0.00523    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 147         |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | -0.98595643 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 261         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 478         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026896972 |\n",
            "|    clip_fraction        | 0.32        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | -0.0314     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 19.1        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0107     |\n",
            "|    reward               | 1.5933386   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 34.8        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 30         |\n",
            "|    time_elapsed         | 494        |\n",
            "|    total_timesteps      | 61440      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02488761 |\n",
            "|    clip_fraction        | 0.245      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.5      |\n",
            "|    explained_variance   | 0.00757    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 65.2       |\n",
            "|    n_updates            | 290        |\n",
            "|    policy_gradient_loss | -0.0173    |\n",
            "|    reward               | 1.5954332  |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 142        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 31         |\n",
            "|    time_elapsed         | 511        |\n",
            "|    total_timesteps      | 63488      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02903176 |\n",
            "|    clip_fraction        | 0.241      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.6      |\n",
            "|    explained_variance   | 0.0419     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 34.8       |\n",
            "|    n_updates            | 300        |\n",
            "|    policy_gradient_loss | -0.00949   |\n",
            "|    reward               | 0.21393205 |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 105        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 527         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016292362 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.0402      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.4        |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.00869    |\n",
            "|    reward               | -1.9982014  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 45.4        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 9620329.69\n",
            "total_reward: 8620329.69\n",
            "total_cost: 260466.85\n",
            "total_trades: 74049\n",
            "Sharpe: 1.129\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 544         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022031048 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0309      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.3        |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0164     |\n",
            "|    reward               | -0.6926965  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 85.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 560         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023266971 |\n",
            "|    clip_fraction        | 0.197       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.106       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 144         |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.00738    |\n",
            "|    reward               | 1.214887    |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 268         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 577         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021608349 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.0315      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 37.6        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.00502    |\n",
            "|    reward               | 0.7790559   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 53.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 598         |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017319236 |\n",
            "|    clip_fraction        | 0.206       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.157       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.64        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    reward               | -3.8811111  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 23.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 123        |\n",
            "|    iterations           | 37         |\n",
            "|    time_elapsed         | 615        |\n",
            "|    total_timesteps      | 75776      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01690711 |\n",
            "|    clip_fraction        | 0.155      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.9      |\n",
            "|    explained_variance   | 0.0701     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 120        |\n",
            "|    n_updates            | 360        |\n",
            "|    policy_gradient_loss | -0.00916   |\n",
            "|    reward               | -0.6844239 |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 191        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 123        |\n",
            "|    iterations           | 38         |\n",
            "|    time_elapsed         | 631        |\n",
            "|    total_timesteps      | 77824      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02698991 |\n",
            "|    clip_fraction        | 0.273      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.9      |\n",
            "|    explained_variance   | 0.131      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 143        |\n",
            "|    n_updates            | 370        |\n",
            "|    policy_gradient_loss | -0.00862   |\n",
            "|    reward               | -13.400137 |\n",
            "|    std                  | 1.06       |\n",
            "|    value_loss           | 178        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 648         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055440433 |\n",
            "|    clip_fraction        | 0.29        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.229       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 31          |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.00724    |\n",
            "|    reward               | -3.741211   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 45.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 664         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014082238 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.181       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 116         |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0176     |\n",
            "|    reward               | 0.03622075  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 158         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 681         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022111297 |\n",
            "|    clip_fraction        | 0.194       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.21        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 157         |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    reward               | 0.22896056  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 263         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 697         |\n",
            "|    total_timesteps      | 86016       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020924803 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.0926      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 198         |\n",
            "|    n_updates            | 410         |\n",
            "|    policy_gradient_loss | -0.00741    |\n",
            "|    reward               | -1.1876119  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 192         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 713         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018821927 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.308       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.39        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.0117     |\n",
            "|    reward               | -1.1503091  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 16.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 729         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020209435 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.0779      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 50.5        |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.00867    |\n",
            "|    reward               | 0.34374893  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 92.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 746         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019379992 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.232       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 58          |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0057     |\n",
            "|    reward               | 1.0899577   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 130         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 762         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020357553 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.481       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.1        |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00697    |\n",
            "|    reward               | -4.620367   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 26.7        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5590160.98\n",
            "total_reward: 4590160.98\n",
            "total_cost: 277954.84\n",
            "total_trades: 74587\n",
            "Sharpe: 0.911\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 778         |\n",
            "|    total_timesteps      | 96256       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022340054 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.34        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 73.3        |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | 1.9421827   |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 124         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 794         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025612283 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.298       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 57.3        |\n",
            "|    n_updates            | 470         |\n",
            "|    policy_gradient_loss | -0.00544    |\n",
            "|    reward               | 9.114032    |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 87.4        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 123        |\n",
            "|    iterations           | 49         |\n",
            "|    time_elapsed         | 811        |\n",
            "|    total_timesteps      | 100352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03336578 |\n",
            "|    clip_fraction        | 0.304      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.5      |\n",
            "|    explained_variance   | 0.0599     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 40.3       |\n",
            "|    n_updates            | 480        |\n",
            "|    policy_gradient_loss | -0.00436   |\n",
            "|    reward               | -0.6591551 |\n",
            "|    std                  | 1.08       |\n",
            "|    value_loss           | 128        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 123        |\n",
            "|    iterations           | 50         |\n",
            "|    time_elapsed         | 827        |\n",
            "|    total_timesteps      | 102400     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03555335 |\n",
            "|    clip_fraction        | 0.229      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.5      |\n",
            "|    explained_variance   | 0.555      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 6.98       |\n",
            "|    n_updates            | 490        |\n",
            "|    policy_gradient_loss | -0.0139    |\n",
            "|    reward               | 0.76138353 |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 27.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 845         |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022626905 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.444       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 86.3        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.0111     |\n",
            "|    reward               | 0.89471865  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 212         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 861         |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026250437 |\n",
            "|    clip_fraction        | 0.236       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.6       |\n",
            "|    explained_variance   | 0.379       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 114         |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.0121     |\n",
            "|    reward               | -6.023672   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 228         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 123        |\n",
            "|    iterations           | 53         |\n",
            "|    time_elapsed         | 877        |\n",
            "|    total_timesteps      | 108544     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03919356 |\n",
            "|    clip_fraction        | 0.255      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.7      |\n",
            "|    explained_variance   | 0.74       |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 8.68       |\n",
            "|    n_updates            | 520        |\n",
            "|    policy_gradient_loss | -0.00761   |\n",
            "|    reward               | 1.8815415  |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 23.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 894         |\n",
            "|    total_timesteps      | 110592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020339482 |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.291       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 138         |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.0099     |\n",
            "|    reward               | 1.1244762   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 164         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 911         |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022854244 |\n",
            "|    clip_fraction        | 0.224       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.326       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 144         |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | 0.00476     |\n",
            "|    reward               | 7.8769064   |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 288         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 927         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028836388 |\n",
            "|    clip_fraction        | 0.295       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.202       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 55.6        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | 0.00184     |\n",
            "|    reward               | 2.7534642   |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 100         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 123        |\n",
            "|    iterations           | 57         |\n",
            "|    time_elapsed         | 944        |\n",
            "|    total_timesteps      | 116736     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.0167926  |\n",
            "|    clip_fraction        | 0.178      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.9      |\n",
            "|    explained_variance   | 0.323      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 151        |\n",
            "|    n_updates            | 560        |\n",
            "|    policy_gradient_loss | -0.00654   |\n",
            "|    reward               | -0.7811658 |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 274        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 960         |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020896845 |\n",
            "|    clip_fraction        | 0.223       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.9       |\n",
            "|    explained_variance   | 0.48        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 105         |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00954    |\n",
            "|    reward               | 0.49232832  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 269         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 976         |\n",
            "|    total_timesteps      | 120832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029780103 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.447       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 84.4        |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.00715    |\n",
            "|    reward               | -1.2001548  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 214         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 992         |\n",
            "|    total_timesteps      | 122880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029841159 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.779       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.6        |\n",
            "|    n_updates            | 590         |\n",
            "|    policy_gradient_loss | -0.00624    |\n",
            "|    reward               | 0.76234597  |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 22.6        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 80\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 10651546.76\n",
            "total_reward: 9651546.76\n",
            "total_cost: 211695.07\n",
            "total_trades: 67839\n",
            "Sharpe: 1.182\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 1008        |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011857234 |\n",
            "|    clip_fraction        | 0.096       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.462       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 93.4        |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.00788    |\n",
            "|    reward               | 0.64578724  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 213         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1024        |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017348705 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.403       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 125         |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.0092     |\n",
            "|    reward               | 4.3886113   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 294         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 123        |\n",
            "|    iterations           | 63         |\n",
            "|    time_elapsed         | 1040       |\n",
            "|    total_timesteps      | 129024     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05761365 |\n",
            "|    clip_fraction        | 0.261      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.1      |\n",
            "|    explained_variance   | 0.526      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 26.2       |\n",
            "|    n_updates            | 620        |\n",
            "|    policy_gradient_loss | 0.00728    |\n",
            "|    reward               | 4.739255   |\n",
            "|    std                  | 1.11       |\n",
            "|    value_loss           | 60.7       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 1057        |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025535967 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.499       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 128         |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | -0.0106     |\n",
            "|    reward               | -0.40438005 |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 204         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1072        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040658336 |\n",
            "|    clip_fraction        | 0.287       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.568       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 150         |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.00207    |\n",
            "|    reward               | -1.0109283  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 308         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 1089        |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013008121 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.3         |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 213         |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.00403    |\n",
            "|    reward               | 6.167982    |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 371         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 67         |\n",
            "|    time_elapsed         | 1105       |\n",
            "|    total_timesteps      | 137216     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02185924 |\n",
            "|    clip_fraction        | 0.19       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.3      |\n",
            "|    explained_variance   | 0.824      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 11.4       |\n",
            "|    n_updates            | 660        |\n",
            "|    policy_gradient_loss | -0.00809   |\n",
            "|    reward               | -1.6173241 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 28.2       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1121        |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020571511 |\n",
            "|    clip_fraction        | 0.211       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.443       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 437         |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.00369    |\n",
            "|    reward               | -0.44523567 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 390         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 1138        |\n",
            "|    total_timesteps      | 141312      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023990382 |\n",
            "|    clip_fraction        | 0.225       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.507       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 60.5        |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.000336   |\n",
            "|    reward               | 1.5340471   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 288         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 1154        |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012016913 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.822       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18.9        |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.00395    |\n",
            "|    reward               | -1.0341524  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 40.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 1170        |\n",
            "|    total_timesteps      | 145408      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022681389 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.563       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 233         |\n",
            "|    n_updates            | 700         |\n",
            "|    policy_gradient_loss | 0.00242     |\n",
            "|    reward               | 1.1215682   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 280         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 1186        |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013235249 |\n",
            "|    clip_fraction        | 0.19        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.575       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 91.6        |\n",
            "|    n_updates            | 710         |\n",
            "|    policy_gradient_loss | 2.51e-05    |\n",
            "|    reward               | -67.112915  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 326         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 73          |\n",
            "|    time_elapsed         | 1202        |\n",
            "|    total_timesteps      | 149504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012156767 |\n",
            "|    clip_fraction        | 0.136       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.49        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 105         |\n",
            "|    n_updates            | 720         |\n",
            "|    policy_gradient_loss | -0.00652    |\n",
            "|    reward               | -1.1820594  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 200         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 74         |\n",
            "|    time_elapsed         | 1218       |\n",
            "|    total_timesteps      | 151552     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01779244 |\n",
            "|    clip_fraction        | 0.156      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.5      |\n",
            "|    explained_variance   | 0.606      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 63.7       |\n",
            "|    n_updates            | 730        |\n",
            "|    policy_gradient_loss | -0.00443   |\n",
            "|    reward               | -2.0968714 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 168        |\n",
            "----------------------------------------\n",
            "day: 2892, episode: 90\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 10486601.25\n",
            "total_reward: 9486601.25\n",
            "total_cost: 137364.26\n",
            "total_trades: 61835\n",
            "Sharpe: 1.109\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 1235        |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012184033 |\n",
            "|    clip_fraction        | 0.134       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.647       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 91.9        |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00419    |\n",
            "|    reward               | 1.2886934   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 297         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 1251        |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010727834 |\n",
            "|    clip_fraction        | 0.0716      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.555       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 229         |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.00619    |\n",
            "|    reward               | -3.6146393  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 265         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 1267        |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031492736 |\n",
            "|    clip_fraction        | 0.266       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.872       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 20.3        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | 0.00123     |\n",
            "|    reward               | 0.83768     |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 37.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 78          |\n",
            "|    time_elapsed         | 1283        |\n",
            "|    total_timesteps      | 159744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029414944 |\n",
            "|    clip_fraction        | 0.222       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.62        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 78.9        |\n",
            "|    n_updates            | 770         |\n",
            "|    policy_gradient_loss | -0.00121    |\n",
            "|    reward               | 1.9844627   |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 324         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 1299        |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013980987 |\n",
            "|    clip_fraction        | 0.18        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.6       |\n",
            "|    explained_variance   | 0.618       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 284         |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.000123   |\n",
            "|    reward               | -5.8907976  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 391         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1315        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011077663 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.804       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29          |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.00458    |\n",
            "|    reward               | 0.047991935 |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 102         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 1331        |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016447362 |\n",
            "|    clip_fraction        | 0.191       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.572       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 155         |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.00231    |\n",
            "|    reward               | -0.4024213  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 298         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 82          |\n",
            "|    time_elapsed         | 1348        |\n",
            "|    total_timesteps      | 167936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011538137 |\n",
            "|    clip_fraction        | 0.0833      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.682       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 149         |\n",
            "|    n_updates            | 810         |\n",
            "|    policy_gradient_loss | -0.00269    |\n",
            "|    reward               | 0.105644    |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 349         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 1364        |\n",
            "|    total_timesteps      | 169984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035315312 |\n",
            "|    clip_fraction        | 0.0894      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.505       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 83.1        |\n",
            "|    n_updates            | 820         |\n",
            "|    policy_gradient_loss | 0.00149     |\n",
            "|    reward               | -2.6561418  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 389         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 1380        |\n",
            "|    total_timesteps      | 172032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024370767 |\n",
            "|    clip_fraction        | 0.329       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.865       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.3        |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | 0.00463     |\n",
            "|    reward               | 1.0607411   |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 48.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 1396        |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016140964 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.589       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 227         |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.00102     |\n",
            "|    reward               | -0.41876623 |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 383         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1412        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013865912 |\n",
            "|    clip_fraction        | 0.094       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.577       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 290         |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | 4.84e-05    |\n",
            "|    reward               | -0.29829496 |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 377         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 87         |\n",
            "|    time_elapsed         | 1429       |\n",
            "|    total_timesteps      | 178176     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00982978 |\n",
            "|    clip_fraction        | 0.0826     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.9      |\n",
            "|    explained_variance   | 0.813      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 37.7       |\n",
            "|    n_updates            | 860        |\n",
            "|    policy_gradient_loss | -0.00556   |\n",
            "|    reward               | 3.8960133  |\n",
            "|    std                  | 1.14       |\n",
            "|    value_loss           | 79.7       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 88         |\n",
            "|    time_elapsed         | 1445       |\n",
            "|    total_timesteps      | 180224     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02759172 |\n",
            "|    clip_fraction        | 0.262      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.9      |\n",
            "|    explained_variance   | 0.628      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 161        |\n",
            "|    n_updates            | 870        |\n",
            "|    policy_gradient_loss | -0.004     |\n",
            "|    reward               | -1.6186426 |\n",
            "|    std                  | 1.14       |\n",
            "|    value_loss           | 285        |\n",
            "----------------------------------------\n",
            "day: 2892, episode: 100\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 11892889.82\n",
            "total_reward: 10892889.82\n",
            "total_cost: 99555.09\n",
            "total_trades: 58719\n",
            "Sharpe: 1.083\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 89          |\n",
            "|    time_elapsed         | 1461        |\n",
            "|    total_timesteps      | 182272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.02321419  |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.66        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 240         |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | 0.00138     |\n",
            "|    reward               | -0.22954614 |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 393         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 124          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 1477         |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041495822 |\n",
            "|    clip_fraction        | 0.0145       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -45.1        |\n",
            "|    explained_variance   | 0.491        |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 186          |\n",
            "|    n_updates            | 890          |\n",
            "|    policy_gradient_loss | -0.0032      |\n",
            "|    reward               | -1.2173266   |\n",
            "|    std                  | 1.15         |\n",
            "|    value_loss           | 372          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 1494        |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.03270653  |\n",
            "|    clip_fraction        | 0.212       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 23.3        |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | 0.00288     |\n",
            "|    reward               | -0.60021013 |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 40.1        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 1510        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025118371 |\n",
            "|    clip_fraction        | 0.279       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.2       |\n",
            "|    explained_variance   | 0.694       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 207         |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | 0.00591     |\n",
            "|    reward               | -1.6359726  |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 371         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 124        |\n",
            "|    iterations           | 93         |\n",
            "|    time_elapsed         | 1526       |\n",
            "|    total_timesteps      | 190464     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01901663 |\n",
            "|    clip_fraction        | 0.0989     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.2      |\n",
            "|    explained_variance   | 0.57       |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 221        |\n",
            "|    n_updates            | 920        |\n",
            "|    policy_gradient_loss | -0.00394   |\n",
            "|    reward               | -4.486595  |\n",
            "|    std                  | 1.15       |\n",
            "|    value_loss           | 453        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 1543        |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011921711 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.2       |\n",
            "|    explained_variance   | 0.826       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 23.7        |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.000287   |\n",
            "|    reward               | -0.18787596 |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 51.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1559        |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.0325021   |\n",
            "|    clip_fraction        | 0.344       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.588       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 75.9        |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | -0.00457    |\n",
            "|    reward               | -0.73419785 |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 301         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 1575        |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025710665 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.69        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 142         |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | -0.00619    |\n",
            "|    reward               | 6.9179215   |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 340         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 1591        |\n",
            "|    total_timesteps      | 198656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028345454 |\n",
            "|    clip_fraction        | 0.242       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.678       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 90.7        |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | 0.00403     |\n",
            "|    reward               | -0.30007705 |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 168         |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 124       |\n",
            "|    iterations           | 98        |\n",
            "|    time_elapsed         | 1608      |\n",
            "|    total_timesteps      | 200704    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0240599 |\n",
            "|    clip_fraction        | 0.219     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -45.4     |\n",
            "|    explained_variance   | 0.527     |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 333       |\n",
            "|    n_updates            | 970       |\n",
            "|    policy_gradient_loss | -0.00428  |\n",
            "|    reward               | 2.2116985 |\n",
            "|    std                  | 1.16      |\n",
            "|    value_loss           | 343       |\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo, \n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JSAHhV4Xc-bh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100, \n",
        "              \"buffer_size\": 1000000, \n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OSRxNYAxdKpU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 2892, episode: 110\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5410297.43\n",
            "total_reward: 4410297.43\n",
            "total_cost: 999.00\n",
            "total_trades: 31812\n",
            "Sharpe: 0.942\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 77       |\n",
            "|    time_elapsed    | 149      |\n",
            "|    total_timesteps | 11572    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 75.7     |\n",
            "|    critic_loss     | 216      |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11471    |\n",
            "|    reward          | 7.616742 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 77       |\n",
            "|    time_elapsed    | 299      |\n",
            "|    total_timesteps | 23144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 43.4     |\n",
            "|    critic_loss     | 30.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23043    |\n",
            "|    reward          | 7.616742 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 77       |\n",
            "|    time_elapsed    | 448      |\n",
            "|    total_timesteps | 34716    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 33.2     |\n",
            "|    critic_loss     | 12.2     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 34615    |\n",
            "|    reward          | 7.616742 |\n",
            "---------------------------------\n",
            "day: 2892, episode: 120\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5410297.43\n",
            "total_reward: 4410297.43\n",
            "total_cost: 999.00\n",
            "total_trades: 31812\n",
            "Sharpe: 0.942\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 77       |\n",
            "|    time_elapsed    | 599      |\n",
            "|    total_timesteps | 46288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 26.7     |\n",
            "|    critic_loss     | 15.5     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 46187    |\n",
            "|    reward          | 7.616742 |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3, \n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xwOhVjqRkCdM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cuda device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K8RSdKCckJyH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 45        |\n",
            "|    time_elapsed    | 252       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 523       |\n",
            "|    critic_loss     | 38.9      |\n",
            "|    ent_coef        | 0.132     |\n",
            "|    ent_coef_loss   | -94.2     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 3.8810892 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 130\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4905600.07\n",
            "total_reward: 3905600.07\n",
            "total_cost: 106364.08\n",
            "total_trades: 61138\n",
            "Sharpe: 0.931\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 46       |\n",
            "|    time_elapsed    | 501      |\n",
            "|    total_timesteps | 23144    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 246      |\n",
            "|    critic_loss     | 564      |\n",
            "|    ent_coef        | 0.0417   |\n",
            "|    ent_coef_loss   | -144     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 23043    |\n",
            "|    reward          | 8.490198 |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 46        |\n",
            "|    time_elapsed    | 749       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 133       |\n",
            "|    critic_loss     | 62.9      |\n",
            "|    ent_coef        | 0.0133    |\n",
            "|    ent_coef_loss   | -162      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 3.1467032 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 140\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3823183.89\n",
            "total_reward: 2823183.89\n",
            "total_cost: 5162.19\n",
            "total_trades: 45298\n",
            "Sharpe: 0.778\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 46        |\n",
            "|    time_elapsed    | 995       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 79.4      |\n",
            "|    critic_loss     | 10.3      |\n",
            "|    ent_coef        | 0.00441   |\n",
            "|    ent_coef_loss   | -119      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 3.3176532 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 47        |\n",
            "|    time_elapsed    | 1226      |\n",
            "|    total_timesteps | 57860     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 46.4      |\n",
            "|    critic_loss     | 7.06      |\n",
            "|    ent_coef        | 0.0017    |\n",
            "|    ent_coef_loss   | -14.6     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 57759     |\n",
            "|    reward          | 4.4624076 |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 1471     |\n",
            "|    total_timesteps | 69432    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 27       |\n",
            "|    critic_loss     | 3.83     |\n",
            "|    ent_coef        | 0.00144  |\n",
            "|    ent_coef_loss   | -3.15    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 69331    |\n",
            "|    reward          | 4.687451 |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac, \n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgGm3dQZfRks"
      },
      "source": [
        "## Save the trained agent\n",
        "Trained agents should have already been saved in the \"trained_models\" drectory after you run the code blocks above.\n",
        "\n",
        "For Colab users, the zip files should be at \"./trained_models\" or \"/content/trained_models\".\n",
        "\n",
        "For users running on your local environment, the zip files should be at \"./trained_models\"."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MRiOtrywfAo1",
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv",
        "Dr49PotrfG01"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
